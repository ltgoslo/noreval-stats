# NorEval Stats - Project Overview

This project contains evaluation results from the **NorEval** benchmark suite (lm-eval-harness v0.4.10) for Norwegian language models. The goal is to produce statistics and plots comparing models.

## Directory Structure

```
noreval-stats/
├── results/              # Cross-model comparison (different models, final checkpoints)
│   ├── NorOLMo-13b/
│   ├── normistral-7b-warm/
│   ├── normistral-11b-warm/
│   ├── normistral-11b-long/
│   └── olmo-2-13B-stage1/
├── NorOLMo_progress/     # Training progress (single model, 33 checkpoints)
│   ├── NorOLMo-step-1000/
│   ├── NorOLMo-step-2000/
│   ├── ...
│   └── NorOLMo-step-33000/
├── docs/                 # Static website (GitHub Pages)
│   ├── index.html
│   ├── app.js
│   ├── style.css
│   └── data.json         # Generated by build_data.py
├── metrics_setup.yaml    # Benchmark config (pretty_name, main_metric, random_baseline, category, metric_scale)
├── build_data.py         # Consolidates result JSONs → docs/data.json
├── check_missing.py      # Validates data completeness
├── .github/workflows/deploy.yml  # Auto-rebuild on push
└── run_interactive.sh
```

## Model Names (as stored in result JSONs)

| Directory Name | model_name (pretrained) | model_name_sanitized |
|---|---|---|
| `norolmo-13b` | `NorOLMo/NorOLMo-step-33000` | `NorOLMo__NorOLMo-step-33000` |
| `normistral-7b-warm` | `models/normistral-7b-warm` | `models__normistral-7b-warm` |
| `normistral-11b-warm` | `models/normistral-11b-warm` | `models__normistral-11b-warm` |
| `normistral-11b-long` | `models/normistral-11b-long` | `models__normistral-11b-long` |
| `olmo-2-13B-stage1` | `models/OLMo-2-1124-13B-stage1-step596057-tokens5001B` | `models__OLMo-2-1124-13B-stage1-step596057-tokens5001B` |

For `NorOLMo_progress/`, each checkpoint uses `NorOLMo/NorOLMo-step-{N}` as the pretrained name.

## NorOLMo Progress Checkpoints

33 checkpoints from step 1000 to step 33000 in increments of 1000.

**Note:** `NorOLMo-step-4000` is missing the `norec_document` benchmark (25 benchmarks instead of 26).

## Result File Path Convention

```
{base}/{model_dir}/{benchmark_name}/{N}-shot/{model_name_sanitized}/results_{timestamp}.json
```

Each benchmark directory contains `0-shot/`, `1-shot/`, and `5-shot/` subdirectories. Inside each shot directory is a subdirectory named after the sanitized model name, containing:
- `results_{timestamp}.json` - aggregate metrics (this is the main file to parse)
- `samples_{benchmark}_{timestamp}.jsonl` (or `samples_{benchmark}_p{N}_{timestamp}.jsonl` for multi-prompt benchmarks) - per-sample predictions

## Benchmarks (35 total)

The canonical list of benchmarks and their main metrics is defined in `metrics_setup.yaml`. All models are evaluated on the same 35 benchmarks (some with `_nno` and `_nob` variants for Nynorsk/Bokmål).

### Multiple-Choice Benchmarks (output_type: "multiple_choice")

| Benchmark | Metrics | Prompt Variants | main_metric | Notes |
|---|---|---|---|---|
| `norbelebele` | `acc`, `acc_norm` | 5 (p0-p4) | `acc` | Reading comprehension (BeleBele) |
| `norcommonsenseqa_nob` | `acc`, `acc_norm` | 5 (p0-p4) | `acc` | Commonsense reasoning (Bokmål) |
| `norcommonsenseqa_nno` | `acc`, `acc_norm` | 5 (p0-p4) | `acc` | Commonsense reasoning (Nynorsk) |
| `noropenbookqa_nob` | `acc`, `acc_norm` | 5 (p0-p4) | `acc` | Open-book QA with science fact (Bokmål) |
| `noropenbookqa_nno` | `acc`, `acc_norm` | 5 (p0-p4) | `acc` | Open-book QA with science fact (Nynorsk) |
| `noropenbookqa_no_fact_nob` | `acc`, `acc_norm` | 5 (p0-p4) | `acc` | Open-book QA without fact (Bokmål) |
| `noropenbookqa_no_fact_nno` | `acc`, `acc_norm` | 5 (p0-p4) | `acc` | Open-book QA without fact (Nynorsk) |
| `nortruthfulqa_mc_nob` | `acc` | 5 (p0-p4) | `acc` | TruthfulQA multiple choice (Bokmål) |
| `nortruthfulqa_mc_nno` | `acc` | 5 (p0-p4) | `acc` | TruthfulQA multiple choice (Nynorsk) |
| `nrk_quiz_qa_nob` | `acc`, `acc_norm` | 5 (p0-p4) | `acc` | NRK quiz QA (Bokmål) |
| `nrk_quiz_qa_nno` | `acc`, `acc_norm` | 5 (p0-p4) | `acc` | NRK quiz QA (Nynorsk) |
| `norec_sentence` | `acc`, `f1` | 5 (p0-p4) | `f1` | Sentiment analysis sentence-level |
| `norec_document` | `acc`, `f1` | 5 (p0-p4) | `f1` | Sentiment analysis document-level |
| `ncb` | `acc` | 1 (single) | `acc` | Norwegian comma benchmark |
| `nocola` | `acc` | 1 (single) | `acc` | Linguistic acceptability |
| `noreval_multiblimp` | `acc`, `acc_norm` | 1 (single) | `acc` | Linguistic acceptability for Northern Sámi (MultiBLiMP) |
| `slide` | `acc` | 5 (p0-p4) | `acc` | Scandinavian language identification |

### Generation Benchmarks (output_type: "generate_until")

| Benchmark | Metrics | Prompt Variants | main_metric | Notes |
|---|---|---|---|---|
| `nortruthfulqa_gen_nob` | `bleu_max/acc/diff`, `rouge1/2/L_max/acc/diff` | 5 (p0-p4) | `rougeL_max` | TruthfulQA generation (Bokmål) |
| `nortruthfulqa_gen_nno` | same as above | 5 (p0-p4) | `rougeL_max` | TruthfulQA generation (Nynorsk) |
| `norquad` | `f1`, `exact_match` | 5 (p0-p4) | `f1` | Reading comprehension (extractive QA) |
| `norsummarize_instruct` | `bleu`, `chrf` | 1 (single) | `bleu` | Summarization (instructed) |
| `norsumm_nob` | `bleu_max`, `bleu_avg`, `rougeL_max`, `rougeL_avg` | 6 (p0-p5) | `rougeL_max` | Summarization (Bokmål) |
| `norsumm_nno` | same as above | 6 (p0-p5) | `rougeL_max` | Summarization (Nynorsk) |
| `norrewrite_instruct` | `bleu`, `chrf` | 1 (single) | `bleu` | Instruction-following (rewriting) |
| `ask_gec` | `exact_match`, `errant_f05` | 5 (p0-p4) | `errant_f05` | Grammar error correction |
| `noridiom_nob` | `em_first`, `em`, `fscore` | 5 (p0-p4) | `em_first` | Idiom completion (Bokmål) |
| `noridiom_nno` | `em_first`, `em`, `fscore` | 5 (p0-p4) | `em_first` | Idiom completion (Nynorsk) |
| `tatoeba_nob_eng` | `bleu`, `chrf` | 4 (p0-p3) | `bleu` | Translation NOB->ENG |
| `tatoeba_eng_nob` | `bleu`, `chrf` | 4 (p0-p3) | `bleu` | Translation ENG->NOB |
| `tatoeba_nno_eng` | `bleu`, `chrf` | 4 (p0-p3) | `bleu` | Translation NNO->ENG |
| `tatoeba_eng_nno` | `bleu`, `chrf` | 4 (p0-p3) | `bleu` | Translation ENG->NNO |
| `tatoeba_nob_sme` | `bleu`, `chrf` | 5 (p0-p4) | `bleu` | Translation NOB->SME (Sami) |
| `tatoeba_sme_nob` | `bleu`, `chrf` | 5 (p0-p4) | `bleu` | Translation SME->NOB (Sami) |
| `norsumm_nob_nno_translation` | `bleu`, `chrf` | 4 (p0-p3) | `bleu` | Translation NOB->NNO |
| `norsumm_nno_nob_translation` | `bleu`, `chrf` | 4 (p0-p3) | `bleu` | Translation NNO->NOB |

### MultiBLiMP (noreval_multiblimp)

A linguistic acceptability benchmark for Northern Sámi based on minimal pairs (Jumelet et al., 2025). Tests subject-verb agreement across two phenomena:

- **Person agreement** (7 subtasks): 1→2, 1→3, 1→2|3, 2→1, 2→3, 3→1, 3→2
- **Number agreement** (6 subtasks): SG→PL, PL→SG, SG→DU, DU→SG, PL→DU, DU→PL

The result JSON contains a group entry `noreval_multiblimp` (micro-averaged `acc`) plus 13 subtask entries like `noreval_multiblimp_1-2`, `noreval_multiblimp_SG-PL`. These are NOT prompt variants (no `_p` prefix). Random baseline is 0.5 (binary choice). In the frontend, subtask metrics appear in the metric selector as optgroups organized by phenomenon category.

## Result JSON Structure

Each `results_{timestamp}.json` file contains:

```json
{
  "results": {
    "{benchmark}_p0": {
      "alias": "...",
      "metric_name,none": value,
      "metric_name_stderr,none": stderr_value
    },
    "{benchmark}_p1": { ... },
    ...
  },
  "configs": { ... },           // Task configs including prompts, output_type
  "n-samples": { ... },         // Sample counts per prompt variant
  "config": {
    "model": "vllm",
    "model_args": { "pretrained": "..." }
  },
  "model_name": "...",
  "model_name_sanitized": "...",
  "lm_eval_version": "0.4.10"
}
```

**Key detail for parsing:** Metric keys in the `results` dict use the format `"metric_name,none"` (with a comma). For benchmarks with prompt variants, each variant (p0, p1, etc.) is a separate entry. For single-prompt benchmarks, the key is just the benchmark name.

## Metric Summary for Quick Reference

The `main_metric` per benchmark is defined in `metrics_setup.yaml`. The metric key in the results JSON is `"{main_metric},none"`. When extracting a single representative score per benchmark, take the **best** (max) main_metric across prompt variants.

Summary of main_metric by benchmark type:
- **Most MC benchmarks**: `acc`
- **norec_sentence, norec_document**: `f1`
- **nortruthfulqa_gen**: `rougeL_max`
- **norquad**: `f1`
- **norsumm_nob, norsumm_nno**: `rougeL_max`
- **ask_gec**: `errant_f05`
- **noridiom**: `em_first` (exact match of first generated word)
- **noreval_multiblimp**: `acc` (micro-averaged across subtasks)
- **translation/summarize/rewrite**: `bleu`

## Language Variants

Benchmarks come in pairs for the two Norwegian written standards:
- `_nob` = Bokmål (the more common standard)
- `_nno` = Nynorsk
- `_sme` = Northern Sámi (tatoeba_nob_sme, tatoeba_sme_nob, noreval_multiblimp)
- Some benchmarks have no suffix (ncb, nocola, norbelebele, ask_gec, norec_*, norsummarize_instruct, norrewrite_instruct, slide, norquad) - these are typically in Bokmål

## Notes for Analysis

- All evaluations were run with vLLM on NVIDIA GH200 120GB GPUs
- Models use `trust_remote_code: true`
- Seeds are fixed: random_seed=0, numpy_seed=1234, torch_seed=1234, fewshot_seed=1234
- Prompt variants (p0-p4) test sensitivity to prompt phrasing; the **best** (max) score across variants is reported
- The `_stderr` values are available for uncertainty quantification

## Interactive Website (docs/)

A static website hosted via GitHub Pages from `docs/` on the main branch.

### Architecture

- `build_data.py` → reads all result JSONs + `metrics_setup.yaml` → writes `docs/data.json` (~143 KB)
- `docs/index.html` + `docs/app.js` + `docs/style.css` → static Plotly.js site that loads `data.json`
- `.github/workflows/deploy.yml` → rebuilds `data.json` on push (when results/ or config change)

### build_data.py Key Details

- **Score extraction**: For each (model, benchmark, shot), finds the latest `results_*.json` (by filename sort) and takes the **max** of `main_metric,none` across all prompt variants (`_p0`, `_p1`, ...). This is `max(values)`, NOT average. For benchmarks with subtasks (e.g. `noreval_multiblimp`), per-subtask metrics are extracted as virtual metric names like `"acc: Person: 1→2"` (base metric + subtask pretty name).
- **Auto-discovery**: Scans `results/` directories — adding a new folder automatically adds a new model. Unknown models fall back to directory name as display label.
- **Model display names**: Hardcoded mapping in `MODEL_DISPLAY_NAMES` dict (e.g., `"NorOLMo-13b"` → `"NorOLMo 13B"`). New models need to be added here for pretty names.
- **Task groups**: Defined in `TASK_GROUPS` dict — pairs of benchmarks shown as grouped bars (NOB/NNO variants, translation direction pairs).
- **NNO benchmarks**: All benchmarks containing `_nno` in their name (12 total). Used for the Nynorsk/Bokmål language aggregation filter.
- **Output path**: `docs/data.json`. The `docs/` dir is created if missing.
- **Dependencies**: Only `pyyaml` (+ stdlib). Runs on Python 3.6+.

### data.json Structure

```json
{
  "metrics_setup": {
    "benchmark_name": {
      "pretty_name": "...",
      "main_metric": "...",
      "random_baseline": 0.0,
      "max_performance": 1.0,  // 1.0 for unit-scale (acc/f1/em/errant_f05), 100.0 for percent-scale (bleu/rougeL_max)
      "category": "...",        // one of categories from metrics_setup.yaml
      "available_metrics": ["metric1", "metric2", ...],  // all metrics available for this benchmark
      "subtasks": { ... }     // optional; for benchmarks with subtask breakdowns (e.g. noreval_multiblimp)
    }
  },
  "task_groups": { "Display Name": { "benchmarks": ["bench_a", "bench_b"], "labels": ["Label A", "Label B"] } },
  "standalone_benchmarks": ["norbelebele", "norquad", ...],
  "nno_benchmarks": ["norcommonsenseqa_nno", ...],
  "model_display_names": { "dir_name": "Pretty Name" },
  "models": { "model_dir": { "benchmark": { "0": score, "1": score, "5": score } } },
  "progress": { "1000": { "benchmark": { "0": score, "1": score, "5": score } }, ... }
}
```

### app.js Key Details

**State variables**: `currentTab` ("comparison"|"progress"), `currentShot` ("0"|"1"|"5"), `currentTaskSelection` (dropdown value), `checkedTasks` (Set of benchmark names).

**Dropdown structure**:
- "All (Aggregate)" → aggregates all checked tasks
- "Aggregate by Category" optgroup → 6 categories from `metrics_setup.yaml`
- "Aggregate by Language" optgroup → Bokmål (all non-NNO) / Nynorsk (all NNO)
- "Individual Tasks" optgroup → task groups + standalone benchmarks, sorted alphabetically

**Rendering logic** (6 chart functions):
- `renderAggregateBarChart()` / `renderAggregateProgressChart()` — normalized scores averaged across `checkedTasks`
- `renderGroupedBarChart(name)` / `renderGroupProgressChart(name)` — paired bars/lines from `task_groups`
- `renderSingleBenchmarkBarChart(bench)` / `renderSingleProgressChart(bench)` — single benchmark

**Normalization** (aggregate views only):
```
normalized = (raw - random_baseline) / (max_performance - random_baseline) * 100
```

**Y-axis scaling**: `computeAggregateYMaxAllShots` and `computeRawYMaxAllShots` compute the max across ALL 3 shot settings so the y-axis stays stable when toggling shots. 15% headroom padding added.

**Checkbox behavior**:
- Always visible, showing all 35 benchmarks individually (NOB/NNO as separate items)
- Selecting any dropdown option pre-checks the relevant benchmarks
- Clicking a checkbox while on an individual task switches dropdown to "All (Aggregate)"
- "Select all" / "Select none" buttons available

**Chart toolbar**: Uses `modeBarButtons` to fully replace default Plotly buttons with two custom download buttons (PNG at 3x scale, SVG). Styled via CSS in `style.css` to match the site theme.

### metrics_setup.yaml Fields

Each benchmark has:
- `pretty_name`, `main_metric`, `random_baseline` — original fields
- `category` — one of: "linguistic knowledge", "language understanding", "world knowledge & reasoning", "generation & summarization", "translation"
- `metric_scale` — `"unit"` (0-1: acc, f1, em, em_first, errant_f05) or `"percent"` (0-100: bleu, rougeL_max, chrf)
- `subtasks` — optional dict of subtask definitions (used by `noreval_multiblimp`); each subtask has `pretty_name` and `description`

### Deployment

1. Run `python3 build_data.py` to regenerate `docs/data.json`
2. GitHub Pages serves from `docs/` on `main` branch
3. GitHub Actions auto-rebuilds on push when relevant files change
